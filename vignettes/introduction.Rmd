---
title: "Counting and viewing words and categories"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

This package consists of R functions to drive the java code in jca tools.  You can find that [here](https://github.com/conjugateprior/jca).  It's a light wrapper and not thoroughly tested, but it might do what you want.  The first task, which is probably the hardest, is to get Java and R working together.  That's covered in the other vignette.  

In this one we exercise the package on a small content analysis example.

## Describing documents

Let's start by describing some documents.  We'll use a replication data set from Bara et al.'s analysis of a parliamentary debate on relaxing the abortion laws that took place in Britain in the 60s.  The data has been scraped from Hansard and each speaker's contributions to the debate concatenated.  Consequently each document is named after the speaker whose contributions it contains.

First we load the package and find the data folder
```{r}
library(rjca)
deb <- system.file("extdata", "debate-by-speaker", package="rjca")
dir(deb) ## list the files 
```
In case you are wondering, the prefix on the speaker names indicates whether they abstained, voted yes or voted no after the debate.

Let's compute some summary statistics
```{r}
desc <- jca_desc(deb)
head(desc) ## top of the file
```
This functions computes the number of words, number of word types, number of words that occurred exactly once, the proportion of all words used that were deployed in this document, the ratio of word types to word tokens, and the number of sentences, for each of the documents. 

All the `jca_` functions run their java in the background and drop the results into a file or folder.  This function returned a data frame but also reported the temporary location where the output landed so you can look at it more closely if you want.

Also common to all the functions is the possibility to add information about the documents, e.g. their file encoding and their locale.  Here we've taken the system default locale.  If we wanted to specify that the document were encoded in KOI8-R (a Russian encoding) then we would instead use
```{r,eval=FALSE}
desc <- jca_desc(deb, encoding='KOI8-R')
```
As it happens this would not affect much since the text is almost completely ASCII characters.  For other documents, specifying the encoding will be vital for recognizing the words in the document.  

It is worth noting that, whatever encoding the documents arrive in, the output of these functions will be UTF-8.  This is taken into account by the package, but you may need to know that if you read in the file and folder output with other programs on a system that does not have a system UTF-8 encoding (due to mistaken setup, deliberate perversity, or MS Windows - but I repeat myself). 

## Counting words

To get a word frequency matrix for these documents we type
```{r}
wfmat <- jca_word(deb)
dim(wfmat)
```
The output is a sparse matrix so we aren't storing the thousands of zero counts that inevitably occur.  If you want to work with it further, remember to load the Matrix package.

Sometimes we'd prefer to do some filtering before constructing the matrix.  This function allows you to remove numbers, currency amounts, stop words (if you provide a list), and to reduce to word stems in several languages (using the snowball stemmer).  
For example, if we wanted to remove numbers and currency and apply a stemmer
```{r}
wfmat2 <- jca_word(deb, no.currency=TRUE, no.numbers=TRUE, stemmer='english')
dim(wfmat2) ## now rather smaller
```

## Counting categories

If we have a content analysis dictionary to hand we can count categories of words rather than words.  This has the additional advantage that we can count phrases too.  The dictionary for the Bara et al. study lives next to the documents
```{r}
dict <- system.file("extdata", "bara-et-al.ykd", package="rjca")
```
This one is in Yoshikoder format, an XML format that represents hierarchically structured content analysis dictionaries designed to work in the Yoshikoder software.  However we can also use Lexicoder dictionaries, VBPRO dictionaries, LIWC  dictionaries, and rather experimentally, Wordstat dictionaries too.  If you are writing such things by hand you'll probably find VBPRO format the most convenient. See below for details.

Let's run this dictionary over the debate
```{r}
debca <- jca_cat(dict, deb)
```

Here's a subset of the data.frame output, skipping some rows and columns to make it easier to read 

|                                | debate| debate_advocacy| debate_medical| debate_moral| WordCount|
|:-------------------------------|------:|---------------:|--------------:|------------:|---------:|
|abs-Mr William Deedes.txt       |    201|              36|             28|           14|      1478|
|abs-Sir John Hobson.txt         |    348|              36|             65|            9|      2808|
|no-Mr Kevin McNamara.txt        |    406|              68|             96|            8|      3009|
|no-Mr Norman St John-Stevas.txt |    318|              63|             50|           32|      2245|
|no-Mr Peter Mahon.txt           |     13|               1|              1|            1|        70|
|no-Mr William Wells.txt         |    354|              61|             73|           21|      2606|

The first column 'debate' contains the count of words or phrases (there are only words in this dictionary) that match any pattern in any category.  The next column 'debate_advocacy' is named to indicate that 'advocacy' is a subcategory of 'debate'.  Note that the counts in all the subcategories of 'debate' will add up to the count in the 'debate' category, and so on nestedly downwards if there were more structure in this dictionary.

The word counts are included so that it is possible to say that e.g. McNamara uses medical vocabulary at a rate of 32 words per thousand (because 96/3009 * 1000 = 31.9), in comparison to Deedes who use them at about 19 per thousand.  

There are some subtleties in using this output.  Read on for a discussion, or just skip to the next major seection see how to make concordances. 

### Hierarchical dictionaries

Because dictionaries are hierarchically structured (and have an extra column with word counts) naive row totals will almost never add up to the number of matches in the document.  This matters when you want to treat the output as a contingency table, and gets more extreme the more nesting there is among the dictionary categories.

For example, assume that the `moral` category has within it two subcategories that distinguish religious and church-related language `church`, from general ethical considerations `ethical`. Further assume that all `moral` patterns are in one or other of these two subcategories.  Then the output would contain two extra columns corresponding to the subcategories

     debate, ... debate_moral, debate_moral_church, debate_moral_ethical, ...
     201,    ... 14,           6,                   8, ...

This structure is convenient if we want to be able to merge or separate the subcategories of `moral` in later analysis, but it does mean that the row of category counts now adds up to 215, not 201.  On the other hand, we are guaranteed that 6 + 8 = 14.

But not even *this* will be true if there are patterns considered to be `moral` but neither `church` nor `ethical`, that is: patterns associated with `moral` but not with any of its subcategories.  If these patterns were matched to 2 words in the first document then the output would look instead like

     debate, ... debate_moral, debate_moral_church, debate_moral_ethical, ...
     201, ...    16,           6,                   8, ...

The upshot is: be careful what parts of the output you pass on to other tools.

### Preparing for later analysis

If you are planning to analyse your data using statistical tools that assume a *contingency table* structure, then the duplicated counts from a hierarchical dictionary are probably not what you want.  In these cases, pick a level of analysis and then remove the columns counting the sub and super categories.

For example, an appropriate contingency table can be constructed from the output constructed above as follows
```{r}
debca.table <- debca[,-c(1,ncol(debca))] # remove root and word count
rowSums(debca.table) == debca[,1] # not always true!
```


For example, if you want to look at the relative emphasis of speakers across the six top-level content analysis categories in this dictionary you should first delete the first column `debate` and the last column `WordCount` and any subcategories of `moral`.  

(And if for some reason you needed an extra column counting 'words that were not captured by any content analysis category' you would want to create it as an extra column by subtracting `debate` from `WordCount` before you got rid of them).

### Pattern matching 

Dictionaries with patterns containing wildcards (basically the `*`) and patterns matching multiple words bring some subtle issues with word counting.  

The first issue is when multiple patterns match the same word token.  For example, the bara dictionary has no wildcards and therefore lists as matches in the `advocacy` category `objection`, `objectionable`, and `objections`.  If we replaced the first two with `objection*` but left `objections` then we would double count every instance of `objections` in a document - once as an exact match to `objections` and then again as a match to `objection*`.  With some care this kind of overlap can be avoided, but it is not always knowable in advance of seeing a document whether there are words that would match one or more patterns.

The issue is avoided by the tools by counting 'token coverage' not 'token matches'.  If, for the sake of the example, `objection*` and `objections` were the *only* two patterns in `advocacy` and they matched e.g. the 120th, 175th, and 210th word in some document, then we would record 3 tokens matched to `advocacy`, even though 6 word to pattern matches were actually made.

Counting coverage rather than matches also solves a similar problem generated by multiple word matches. If we counted matches using the patterns `united kingdom` and `united` then every instance of 'united' that preceded kingdom would be counted twice - once for matching the first half of `united kingdom` and once for matching `united`.  Adding wildcards make this even harder to avoid the examining patterns.  As before, if `united kingdom` matches the 25th and 26th word tokens in a document then `united` also matches the 25th, but only the number of tokens covered by patterns in the category are recorded - in this case 2.  
 
### Caveats about pattern matching

Counting token coverage rather than matches removes the risk of double counting within each category.  It also removes the risk of double counting within a hierarchy of nested categories (technically the identities of the matched word tokens are 'percolated' up the category hierarchy from the lowest level).  However, it does not prevent a double counting of a word by two categories at the same level.  

For example, if the `church` and `ethical` subcategories of `moral` contained patterns that matched the same words, then each instance of those words in a document would increment both subcategories.  This is not, in general, possible to prevent.  (Sometimes it is not even problematic, depending on the purpose to which the dictionary is put).  Note, however, that the counts associated with their supercategory `moral` would be correct and would not contain overcounts. 

### Old school pattern matching

If for some reason you liked the possibility of double-counting words, perhaps because you are replicating the analysis of someone who used a tool that did things that way, then you can set the `old.matching` parameter to TRUE.

## Looking at words and categories in context

If we want to see what sorts of things the dictionary is picking up in the documents we can arrange all the category matches in their local context.  This is a form of concordance, or keyword in context (KWIC).  Let's have a look at the medical category.
```{r}
debconc <- jca_conc(deb, dictionary=dict, category='medical')
head(debconc)
```
The words we are matching are aligned in the middle of the second column and the document in which they occur is noted for each instance in the left column.  Here
the words we are looking at in context are: 'doctor', 'physical', 'mental', 'medical' and 'doctors'.

If we are more interested in just one of these words, or in a completely new phrase we can skip the dictionary part and just hand in a pattern directly.  Here we look at how each document uses a phrase
```{r}
debconc2 <- jca_conc(deb, pattern='medical profession')
head(debconc2)
```
Searching instead for the pattern `'medical *'` will show all pairs of words where the first is 'medical'.  This might be useful to see how many uses of 'medical' are not about the profession itself but rather about medical reasoning, medical procedures, and other staff.  Very few, it turns out.

The asterisk is a wildcard that can be embedded in strings too, e.g. `'profess*'` will pick up all words starting with these letters.

## Dictionary formats

The `bara-et-al.ykd` dictionary is saved from the Yoshikoder content analysis software, but if you don't use that application you probably have your dictionary in a different format. The `jca_cat` and `jca_conc` functions accept dictionary files in the following additional formats, recognizing each from the suffix of the file

 * VBPro (suffix `.vbpro`)
 * Lexicoder (suffix `.lcd`)
 * LIWC (suffix: `.dic`)
 * Wordstat (suffix `.CAT`)

The VBPro format is particularly convenient for throwing together a dictionary in a hurry. Such dictionaries look like

    >>Advocacy<<
    object*
    override
    >>Religion<<
    relig*
    catholic church

where the category names are anything between `>>` and `<<` and the words underneath are word or phrase patterns to match, one per line. You can write one of these in any text editor and hand it to the function as a dictionary (to be quite sure you're matching what you think you're matching, save the dictionary file as 'UTF-8'). 

This format is reasonably flexible; you can use wildcards and multiword patterns, but the dictionary has no nested categories so you'll have to fake those with your category naming convention.
